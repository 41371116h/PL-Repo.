{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPshCS5jnhG3yncaeJj3b+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371116h/PL-Repo./blob/main/HW4%E5%90%ABAI%E6%91%98%E8%A6%81_(PTT_China_Drama).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ•¸ï¸ PTT China-Drama çˆ¬èŸ² + ä»»å‹™æ•´åˆ + å…§æ–‡æŠ“å– + æ–‡å­—åˆ†æï¼ˆå«å¯«å› Google Sheet & AI æ‘˜è¦ï¼‰\n",
        "# ğŸš¨ Gemini é…ç½®å€å¡Šå·²å®Œå…¨æ›¿æ›ç‚ºç”¨æˆ¶æŒ‡å®šçš„å¯«æ³• ğŸš¨\n",
        "# ============================================\n",
        "# å®‰è£ï¼ˆColab åŸ·è¡Œä¸€æ¬¡ï¼‰\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 python-dateutil jieba google-generativeai\n",
        "\n",
        "# -------------------------\n",
        "import os, time, uuid, re, datetime\n",
        "import requests, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from collections import Counter, defaultdict\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "import gspread\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "from io import StringIO\n",
        "\n",
        "# =========================\n",
        "# æ™‚é–“å·¥å…·\n",
        "# =========================\n",
        "def tznow():\n",
        "    return datetime.datetime.now().astimezone()\n",
        "\n",
        "# =========================\n",
        "# å¸¸æ•¸ / è¡¨é ­\n",
        "# =========================\n",
        "CLIPS_HEADER = [\"clip_id\",\"url\",\"selector\",\"text\",\"href\",\"created_at\",\"added_to_task\"]\n",
        "TASKS_HEADER = [\"id\",\"task\",\"status\",\"priority\",\"est_min\",\"start_time\",\"end_time\",\n",
        "                \"actual_min\",\"pomodoros\",\"due_date\",\"labels\",\"notes\",\n",
        "                \"created_at\",\"updated_at\",\"completed_at\",\"planned_for\"]\n",
        "PTT_HEADER = [\"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\"fetched_at\",\"content\"]\n",
        "\n",
        "# =========================\n",
        "# Google Sheets åˆå§‹åŒ–ï¼ˆColab èªè­‰ï¼‰\n",
        "# =========================\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    # æŠŠä½ çš„è©¦ç®—è¡¨ URL æ”¾é€™è£¡\n",
        "    SHEET_URL = \"https://docs.google.com/spreadsheets/d/1GScHTHISiioV89XO5twgGl-IpaYG-0nMwhLRTizSpNI/edit#gid=938437500\"\n",
        "    sh = gc.open_by_url(SHEET_URL)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Google Sheets èªè­‰æˆ–é€£ç·šå¤±æ•—: {e}\")\n",
        "    # å‰µå»ºè™›æ“¬ç‰©ä»¶é˜²æ­¢å´©æ½°\n",
        "    class MockWorksheet:\n",
        "        def get_all_records(self): return []\n",
        "        def get_all_values(self): return []\n",
        "        def update(self, *args, **kwargs): pass\n",
        "        def clear(self): pass\n",
        "    ws_clips = MockWorksheet()\n",
        "    ws_analysis = MockWorksheet()\n",
        "\n",
        "\n",
        "# ç¢ºä¿å·¥ä½œè¡¨å­˜åœ¨ï¼ˆ\"çˆ¬èŸ²\"ï¼‰\n",
        "def ensure_ws(name, headers):\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=name, rows=\"2000\", cols=str(len(headers)+10))\n",
        "        ws.update([headers])\n",
        "    # å¦‚æœæ²’æœ‰è¡¨é ­å°±è£œä¸Š\n",
        "    vals = ws.get_all_values()\n",
        "    if not vals or (vals and vals[0] != headers):\n",
        "        ws.clear()\n",
        "        ws.update([headers])\n",
        "    return ws\n",
        "\n",
        "ws_clips = ensure_ws(\"çˆ¬èŸ²\", PTT_HEADER)            # å„²å­˜æŠ“åˆ°çš„æ–‡ç« ï¼ˆå« contentï¼‰\n",
        "ws_analysis = ensure_ws(\"çˆ¬èŸ²_analysis\", [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"])\n",
        "\n",
        "# =========================\n",
        "# Gemini API è¨­å®š (å®Œå…¨ä¾ç…§ç”¨æˆ¶æŒ‡å®šçš„ç•ªèŒ„é˜å¯«æ³•)\n",
        "# =========================\n",
        "model = None # å°‡å…¨åŸŸè®Šæ•¸åç¨±è¨­å®šç‚º model\n",
        "try:\n",
        "    # !!! è«‹æ›¿æ›ç‚ºæ‚¨çœŸå¯¦çš„ API Key !!!\n",
        "    # å»ºè­°ä½¿ç”¨ Colab Secret æˆ–ç’°å¢ƒè®Šæ•¸å„²å­˜\n",
        "    GEMINI_API_KEY = \"AIzaSyAWyvSMGkAgiTMSdE8TEId8IFDw0OD46io\"\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    # ä½¿ç”¨ Flash æ¨¡å‹ï¼Œå®ƒé€Ÿåº¦å¿«ã€æˆæœ¬ä½ï¼Œé©åˆè¦åŠƒä»»å‹™\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    print(\"âœ… Gemini API é…ç½®æˆåŠŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Gemini API é…ç½®å¤±æ•—: {e}\")\n",
        "\n",
        "# =========================\n",
        "# PTT çˆ¬èŸ²å‡½å¼\n",
        "# =========================\n",
        "PTT_INDEX = \"https://www.ptt.cc/bbs/China-Drama/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    for a in soup.select(\"div.btn-group-paging a.btn.wide\"):\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(span):\n",
        "    if not span:\n",
        "        return 0\n",
        "    txt = span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def extract_post_list_from_index(url):\n",
        "    soup = _get_soup(url)\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url_post = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\"title\": title, \"url\": url_post, \"author\": author, \"date\": date, \"nrec\": nrec})\n",
        "    prev = _get_prev_index_url(soup)\n",
        "    return posts, prev\n",
        "\n",
        "# æŠ“ index_pages é çš„æ–‡ç« åˆ—è¡¨ï¼ˆæ¡å»é‡ï¼‰\n",
        "def crawl_index_pages(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    url = PTT_INDEX\n",
        "    all_rows = []\n",
        "    try:\n",
        "        seen = set([r['url'] for r in ws_clips.get_all_records()])\n",
        "    except Exception:\n",
        "        seen = set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        try:\n",
        "            posts, prev = extract_post_list_from_index(url)\n",
        "        except Exception as e:\n",
        "            return f\"âš ï¸ å–å¾— index å¤±æ•—ï¼š{e}\"\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and keyword not in p[\"title\"]:\n",
        "                continue\n",
        "            if p[\"url\"] in seen:\n",
        "                continue\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": p[\"title\"][:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": \"\"\n",
        "            })\n",
        "            seen.add(p[\"url\"])\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "    if all_rows:\n",
        "        df_new = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        # append to sheet\n",
        "        existing = get_as_dataframe(ws_clips, evaluate_formulas=False)\n",
        "        existing = existing.dropna(how=\"all\")\n",
        "        if existing is None or existing.empty:\n",
        "            combined = df_new\n",
        "        else:\n",
        "            combined = pd.concat([existing, df_new], ignore_index=True)\n",
        "        set_with_dataframe(ws_clips, combined)\n",
        "        return f\"âœ… å–å¾— {len(all_rows)} ç¯‡æ–‡ç« ï¼ˆå¯«å…¥ 'çˆ¬èŸ²'ï¼‰\"\n",
        "    return \"â„¹ï¸ ç„¡æ–°æ–‡ç« \"\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # ç§»é™¤æ¨æ–‡å€èˆ‡ meta\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\"\n",
        "    for m in main.select(\"div.article-metaline, div.article-metaline-right\"):\n",
        "        m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def fetch_and_write_contents(limit_per_run=50):\n",
        "    # å¾ sheet è®€å‡ºå°šæœªæœ‰ content çš„ urlï¼ˆæˆ– content ç©ºç™½ï¼‰\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df.empty:\n",
        "        return \"âš ï¸ 'çˆ¬èŸ²' å·¥ä½œè¡¨æ²’æœ‰è³‡æ–™\"\n",
        "    to_fetch = df[df[\"content\"].apply(lambda x: not bool(str(x).strip()))]\n",
        "    if to_fetch.empty:\n",
        "        return \"â„¹ï¸ æ²’æœ‰å¾…æŠ“å–å…§æ–‡\"\n",
        "    updated = 0\n",
        "    for idx, row in to_fetch.head(limit_per_run).iterrows():\n",
        "        url = row[\"url\"]\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "            content = _clean_ptt_content(soup)\n",
        "        except Exception as e:\n",
        "            content = f\"FETCH_ERROR: {e}\" # è¨˜éŒ„éŒ¯èª¤\n",
        "        df.loc[idx, \"content\"] = content\n",
        "        df.loc[idx, \"fetched_at\"] = tznow().isoformat()\n",
        "        updated += 1\n",
        "        time.sleep(0.5)\n",
        "    set_with_dataframe(ws_clips, df)\n",
        "    return f\"âœ… å·²æ›´æ–° {updated} ç¯‡æ–‡ç« çš„å…§æ–‡åˆ° 'çˆ¬èŸ²' å·¥ä½œè¡¨\"\n",
        "\n",
        "def load_sheet_preview(n=100):\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df is None:\n",
        "        return pd.DataFrame()\n",
        "    return df.head(n)\n",
        "\n",
        "# =========================\n",
        "# æ–‡å­—åˆ†æï¼ˆä½¿ç”¨ Gemini API é€²è¡Œæ‘˜è¦ï¼‰\n",
        "# =========================\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def simple_tokenize_cn(text):\n",
        "    # åªä¿ç•™ä¸­æ–‡èˆ‡è‹±æ•¸ï¼Œç„¶å¾Œç”¨ jieba åˆ‡è©\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", str(text))\n",
        "    toks = [w for w in jieba.lcut(text) if len(w.strip())>0]\n",
        "    return toks\n",
        "\n",
        "def analyze_and_write(topn=20, min_df=1, use_tfidf=False):\n",
        "    # è®€å– sheet 'çˆ¬èŸ²' çš„ content æ¬„\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    texts = df[\"content\"].astype(str).tolist()\n",
        "    # æ’é™¤ç©ºå…§å®¹æˆ–éŒ¯èª¤è¨Šæ¯ (FETCH_ERROR)\n",
        "    docs = [t for t in texts if t.strip() and not t.startswith(\"FETCH_ERROR\")]\n",
        "    if not docs:\n",
        "        return \"âš ï¸ æ²’æœ‰å¯åˆ†æçš„æ–‡å­—ï¼ˆ'çˆ¬èŸ²' å·¥ä½œè¡¨çš„ content æ¬„ï¼‰\", pd.DataFrame(), \"\"\n",
        "\n",
        "    # ç´”æ–‡å­—åˆ†æéƒ¨åˆ† (è©é »/å­—æ•¸)\n",
        "    counts = [len(re.findall(r\"[\\u4e00-\\u9fffA-Za-z0-9]\", d)) for d in docs]\n",
        "    total_words = sum(counts)\n",
        "    avg_words = round(total_words / len(counts), 2)\n",
        "    token_docs = [simple_tokenize_cn(d) for d in docs]\n",
        "    freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        freq.update([t for t in toks if re.search(r'[\\u4e00-\\u9fff]', t)])  # å„ªå…ˆä¸­æ–‡è©\n",
        "    topn = int(topn)\n",
        "    common = freq.most_common(topn)\n",
        "    top_words_str = \"; \".join([f\"{w}:{c}\" for w,c in common])\n",
        "\n",
        "    # TF-IDF å¯é¸ï¼ˆè¨ˆç®—ä½†ä¸å½±éŸ¿ top_words_strï¼‰\n",
        "    if use_tfidf:\n",
        "        try:\n",
        "            vec = TfidfVectorizer(tokenizer=simple_tokenize_cn, lowercase=False, min_df=min_df)\n",
        "            vec.fit_transform(docs)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ==========================================================\n",
        "    # AI ç”Ÿæˆæ‘˜è¦ (ä½¿ç”¨å…¨åŸŸè®Šæ•¸ model)\n",
        "    # ==========================================================\n",
        "    ai_summary = \"âš ï¸ AI æ‘˜è¦æœªåŸ·è¡Œ\"\n",
        "    ai_conclusion = \"\"\n",
        "\n",
        "    global model # ä½¿ç”¨ç•ªèŒ„é˜å°ˆæ¡ˆè¨­å®šçš„å…¨åŸŸè®Šæ•¸ model\n",
        "    if model is not None:\n",
        "        try:\n",
        "            all_text_preview = \"\\n---\\n\".join(docs)[:10000]  # æ“·å–å‰ 10000 å­—çµ¦æ¨¡å‹\n",
        "\n",
        "            # ä½¿ç”¨æ‚¨æŒ‡å®šçš„ Prompt æ ¼å¼\n",
        "            prompt = f\"\"\"è«‹ç”¨ä¸­æ–‡ï¼Œæ ¹æ“šä»¥ä¸‹å—è¨ªè€… / æ–‡ç« å…§å®¹ï¼Œè¼¸å‡ºï¼š\n",
        "1) äº”å¥ç°¡çŸ­æ´å¯Ÿï¼ˆæ¯å¥ä¸€è¡Œï¼Œæœ€å¤š 30å­—/å¥ï¼‰\n",
        "2) ä¸€æ®µç´„ 120 å­—çš„çµè«–ï¼ˆç¸½çµæƒ…ç·’èˆ‡ä¸»è¦è¶¨å‹¢ï¼‰ã€‚\n",
        "å…§å®¹ï¼š\n",
        "{all_text_preview}\n",
        "\"\"\"\n",
        "            # å‘¼å« generate_content\n",
        "            resp = model.generate_content(prompt, request_options={\"timeout\": 60})\n",
        "            out = resp.text.strip()\n",
        "\n",
        "            # å˜—è©¦å°‡è¼¸å‡ºåˆ‡åˆ†æˆ summary å’Œ conclusion (ç°¡å–®åˆ‡åˆ†)\n",
        "            parts = out.split('\\n\\n', 1) # åªåˆ‡åˆ†ä¸€æ¬¡\n",
        "\n",
        "            if len(parts) == 2:\n",
        "                ai_summary = parts[0].strip()\n",
        "                ai_conclusion = parts[1].strip()\n",
        "            else:\n",
        "                ai_summary = out.strip()\n",
        "                ai_conclusion = \"ï¼ˆç„¡æ³•å¾ AI è¼¸å‡ºä¸­åˆ‡åˆ†å‡ºæ˜ç¢ºçš„çµè«–æ®µè½ï¼‰\"\n",
        "\n",
        "        except Exception as e:\n",
        "            ai_summary = f\"âš ï¸ Gemini å‘¼å«å¤±æ•—ï¼š{e}\"\n",
        "            ai_conclusion = \"\"\n",
        "\n",
        "    # å›å¯« ws_analysisï¼ˆæ–°ä¸€åˆ—ï¼‰\n",
        "    analysis_row = {\n",
        "        \"analysis_time\": tznow().isoformat(),\n",
        "        \"n_docs\": len(docs),\n",
        "        \"total_words\": total_words,\n",
        "        \"avg_words\": avg_words,\n",
        "        \"top_words\": top_words_str,\n",
        "        \"ai_summary\": ai_summary,\n",
        "        \"ai_conclusion\": ai_conclusion\n",
        "    }\n",
        "\n",
        "    analysis_header = [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"]\n",
        "    df_an = get_as_dataframe(ws_analysis, evaluate_formulas=False).fillna(\"\")\n",
        "\n",
        "    if df_an.empty or list(df_an.columns) != analysis_header:\n",
        "        ws_analysis.clear()\n",
        "        ws_analysis.update([analysis_header])\n",
        "        df_an_new = pd.DataFrame([analysis_row], columns=analysis_header)\n",
        "    else:\n",
        "        df_an_new = pd.concat([df_an, pd.DataFrame([analysis_row], columns=analysis_header)], ignore_index=True)\n",
        "    set_with_dataframe(ws_analysis, df_an_new)\n",
        "\n",
        "    # ä¹ŸæŠŠ top words å¯«åˆ°ä¸€å€‹ sheet \"çˆ¬èŸ²_top_words\"ï¼ˆè¦†è“‹ï¼‰\n",
        "    top_ws = ensure_ws(\"çˆ¬èŸ²_top_words\", [\"word\",\"count\"])\n",
        "    top_df = pd.DataFrame(common, columns=[\"word\",\"count\"])\n",
        "    set_with_dataframe(top_ws, top_df)\n",
        "\n",
        "    # å°‡ summary å’Œ conclusion åˆä½µé¡¯ç¤ºåœ¨ Gradio çš„ ai_out\n",
        "    full_ai_output = f\"ã€æ´å¯Ÿã€‘\\n{ai_summary}\\n\\nã€çµè«–ã€‘\\n{ai_conclusion}\"\n",
        "\n",
        "    return {\n",
        "        \"message\": f\"âœ… åˆ†æå®Œæˆï¼š{len(docs)} ç¯‡ï¼›ç¸½å­—æ•¸ {total_words}ï¼›å¹³å‡ {avg_words}\",\n",
        "        \"top_df\": top_df,\n",
        "        \"ai_summary\": full_ai_output\n",
        "    }\n",
        "\n",
        "# Gradio Wrapper for analysis (èª¿æ•´è¼¸å‡º)\n",
        "def run_analysis(topn, use_tfidf):\n",
        "    res = analyze_and_write(topn, min_df=1, use_tfidf=use_tfidf)\n",
        "    if isinstance(res, dict):\n",
        "        return res[\"message\"], res[\"top_df\"], res[\"ai_summary\"]\n",
        "    return res, pd.DataFrame(), f\"åˆ†æå¤±æ•—: {res}\"\n",
        "\n",
        "# =========================\n",
        "# Gradio UI\n",
        "# =========================\n",
        "# Keep in-memory clip/task tables for Gradio display (your original)\n",
        "clips_df_local = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "tasks_df_local = pd.DataFrame(columns=TASKS_HEADER)\n",
        "\n",
        "with gr.Blocks(title=\"PTT Crawler & Task Integrator (extended)\") as demo:\n",
        "    gr.Markdown(\"## ğŸ•¸ï¸ PTT China-Drama çˆ¬èŸ² + å…§æ–‡æŠ“å– + æ–‡å­—åˆ†æï¼ˆå« Gemini AI æ‘˜è¦ï¼‰\")\n",
        "\n",
        "    with gr.Tab(\"Crawler (index)\"):\n",
        "        url = gr.Textbox(label=\"èµ·å§‹ Index URL\", value=PTT_INDEX)\n",
        "        pages = gr.Number(label=\"å‘ä¸ŠæŠ“å¹¾é  (index pages)\", value=3, precision=0)\n",
        "        min_push = gr.Number(label=\"æœ€å°‘æ¨æ–‡æ•¸ (min_push)\", value=0, precision=0)\n",
        "        keyword = gr.Textbox(label=\"æ¨™é¡Œé—œéµå­—éæ¿¾ (é¸å¡«)\", value=\"\")\n",
        "        btn_index = gr.Button(\"ğŸ“„ æŠ“ Indexï¼ˆå¤šé ï¼‰\")\n",
        "        out_index = gr.Markdown()\n",
        "        btn_index.click(fn=crawl_index_pages, inputs=[pages, min_push, keyword], outputs=[out_index])\n",
        "\n",
        "    with gr.Tab(\"æŠ“å…§æ–‡ï¼ˆå¯«å› Sheetï¼‰\"):\n",
        "        btn_fetch = gr.Button(\"ğŸ“¥ æŠ“å–å°šæœªæŠ“å…§æ–‡çš„æ–‡ç« ï¼ˆå¯«å› 'çˆ¬èŸ²'ï¼‰\")\n",
        "        out_fetch = gr.Markdown()\n",
        "        btn_fetch.click(fn=fetch_and_write_contents, inputs=[gr.Number(value=50, visible=False)], outputs=[out_fetch])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        btn_show_sheet = gr.Button(\"ğŸ“‚ å¾ 'çˆ¬èŸ²' å·¥ä½œè¡¨è®€å‡ºä¸¦é¡¯ç¤º (show)\")\n",
        "        sheet_table = gr.Dataframe(value=pd.DataFrame(), interactive=True)\n",
        "\n",
        "        btn_show_sheet.click(fn=load_sheet_preview, outputs=[sheet_table])\n",
        "\n",
        "    with gr.Tab(\"Add to Tasks\"):\n",
        "        clip_ids = gr.Textbox(label=\"è¦åŠ å…¥ä»»å‹™çš„ post_idï¼ˆå¤šå€‹ä»¥é€—è™Ÿåˆ†éš”ï¼‰\")\n",
        "        default_priority = gr.Radio([\"H\", \"M\", \"L\"], value=\"M\", label=\"é è¨­å„ªå…ˆåº¦\")\n",
        "        est_min = gr.Number(value=25, precision=0, label=\"é ä¼°æ™‚é–“ï¼ˆåˆ†é˜ï¼‰\")\n",
        "        btn_add = gr.Button(\"â• åŠ å…¥ä»»å‹™\")\n",
        "        msg_add = gr.Markdown()\n",
        "        grid_tasks = gr.Dataframe(value=tasks_df_local, label=\"ä»»å‹™æ¸…å–®\", interactive=True)\n",
        "\n",
        "        # local add function\n",
        "        def add_clips_local(clip_ids, default_priority, est_min):\n",
        "            global tasks_df_local\n",
        "            if not clip_ids:\n",
        "                return \"âš ï¸ è«‹è¼¸å…¥ post_id\", load_sheet_preview(), tasks_df_local\n",
        "            ids = [c.strip() for c in clip_ids.split(\",\") if c.strip()]\n",
        "\n",
        "            df_sheet = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "            sel = df_sheet[df_sheet[\"post_id\"].isin(ids)] if not df_sheet.empty else pd.DataFrame()\n",
        "\n",
        "            if sel.empty:\n",
        "                return \"âš ï¸ æ²’æœ‰åŒ¹é…åˆ°ä»»ä½• post_id\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "            _now = tznow().isoformat()\n",
        "            new_tasks = []\n",
        "            for _, r in sel.iterrows():\n",
        "                title = r.get(\"title\", \"\") or \"ï¼ˆæœªå‘½åï¼‰\"\n",
        "                note = f\"PTT é€£çµï¼š{r.get('url','')}\\nä½œè€…ï¼š{r.get('author','')}\\nåŸå§‹ IDï¼š{r.get('post_id','')}\"\n",
        "                new_tasks.append({\n",
        "                    \"id\": str(uuid.uuid4())[:8],\n",
        "                    \"task\": title[:120],\n",
        "                    \"status\": \"todo\",\n",
        "                    \"priority\": default_priority or \"M\",\n",
        "                    \"est_min\": int(est_min) if est_min else 25,\n",
        "                    \"start_time\": \"\",\n",
        "                    \"end_time\": \"\",\n",
        "                    \"actual_min\": 0,\n",
        "                    \"pomodoros\": 0,\n",
        "                    \"due_date\": \"\",\n",
        "                    \"labels\": \"from:ptt\",\n",
        "                    \"notes\": note,\n",
        "                    \"created_at\": _now,\n",
        "                    \"updated_at\": _now,\n",
        "                    \"completed_at\": \"\",\n",
        "                    \"planned_for\": \"\"\n",
        "                })\n",
        "            if new_tasks:\n",
        "                tasks_df_local = pd.concat([tasks_df_local, pd.DataFrame(new_tasks, columns=TASKS_HEADER)], ignore_index=True)\n",
        "                return f\"âœ… å·²åŠ å…¥ {len(new_tasks)} ç­†ä»»å‹™ (local)\", load_sheet_preview(), tasks_df_local\n",
        "            return \"âš ï¸ æ²’æœ‰å¯åŠ å…¥çš„é …ç›®\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "\n",
        "        btn_add.click(fn=add_clips_local, inputs=[clip_ids, default_priority, est_min], outputs=[msg_add, sheet_table, grid_tasks])\n",
        "\n",
        "    with gr.Tab(\"ğŸ§  Text Analysis (Gemini)\"):\n",
        "        topn = gr.Number(label=\"Top N ç†±è©\", value=20, precision=0)\n",
        "        use_tfidf = gr.Checkbox(label=\"ä½¿ç”¨ TF-IDF (è£œå……æ’åº)\", value=False)\n",
        "        btn_analyze = gr.Button(\"ğŸ” åˆ†æï¼ˆå¾ 'çˆ¬èŸ²' è®€å– contentï¼Œä¸¦å¯«å› analysisï¼‰\")\n",
        "        result_msg = gr.Markdown()\n",
        "        top_table = gr.Dataframe()\n",
        "        # AI è¼¸å‡ºæ¬„ä½èª¿æ•´ç‚º Lines=10\n",
        "        ai_out = gr.Textbox(label=\"Gemini AI æ‘˜è¦ (æ´å¯Ÿ + çµè«–)\", lines=10, interactive=False)\n",
        "\n",
        "        btn_analyze.click(fn=run_analysis, inputs=[topn, use_tfidf], outputs=[result_msg, top_table, ai_out])\n",
        "\n",
        "# ====== å•Ÿå‹• App ======\n",
        "if __name__ == \"__main__\":\n",
        "    # å®Œå…¨ä½¿ç”¨æ‚¨æŒ‡å®šçš„åƒæ•¸\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "2HfAA2yuS473",
        "outputId": "f156eb35-b245-4395-a4af-a6ef7e97eee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Gemini API é…ç½®æˆåŠŸã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f248fe8b3c58851991.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f248fe8b3c58851991.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.542 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.542 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    }
  ]
}