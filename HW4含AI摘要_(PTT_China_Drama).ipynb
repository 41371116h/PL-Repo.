{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371116h/PL-Repo./blob/main/HW4%E5%90%ABAI%E6%91%98%E8%A6%81_(PTT_China_Drama).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW4含AI摘要 (PTT China-Drama)（作業四）\n",
        "- 目標：把PTT China-Drama的文章用爬蟲抓下來，包含標題以及內文且可以自行設定想要抓取的頁數，並顯示熱詞分析表格，最後加入Gemini建議及摘要，並顯示在Gradio以及google sheet上\n",
        "- AI 點子（可選）：熱詞分析以及AI摘要建議\n",
        "- Sheet 欄位：post_id, title,url, date,author, nrec, created_at, fetched_at, content\n",
        "- GoogleSheet: https://docs.google.com/spreadsheets/d/1GScHTHISiioV89XO5twgGl-IpaYG-0nMwhLRTizSpNI/edit?pli=1&gid=0#gid=0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AtVS89ekCCcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 🕸️ PTT China-Drama 爬蟲 + 任務整合 + 內文抓取 + 文字分析（含寫回 Google Sheet & AI 摘要）\n",
        "# 🚨 Gradio Dataframe height 參數錯誤已修復 🚨\n",
        "# ============================================\n",
        "# 安裝（Colab 執行一次）\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 python-dateutil jieba google-generativeai\n",
        "\n",
        "# -------------------------\n",
        "import os, time, uuid, re, datetime\n",
        "import requests, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from collections import Counter, defaultdict\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "import gspread\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "from io import StringIO\n",
        "\n",
        "# =========================\n",
        "# 時間工具\n",
        "# =========================\n",
        "def tznow():\n",
        "    return datetime.datetime.now().astimezone()\n",
        "\n",
        "# =========================\n",
        "# 常數 / 表頭\n",
        "# =========================\n",
        "CLIPS_HEADER = [\"clip_id\",\"url\",\"selector\",\"text\",\"href\",\"created_at\",\"added_to_task\"]\n",
        "TASKS_HEADER = [\"id\",\"task\",\"status\",\"priority\",\"est_min\",\"start_time\",\"end_time\",\n",
        "                \"actual_min\",\"pomodoros\",\"due_date\",\"labels\",\"notes\",\n",
        "                \"created_at\",\"updated_at\",\"completed_at\",\"planned_for\"]\n",
        "PTT_HEADER = [\"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\"fetched_at\",\"content\"]\n",
        "\n",
        "# =========================\n",
        "# Google Sheets 初始化（Colab 認證）\n",
        "# =========================\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    # 把你的試算表 URL 放這裡\n",
        "    SHEET_URL = \"https://docs.google.com/spreadsheets/d/1GScHTHISiioV89XO5twgGl-IpaYG-0nMwhLRTizSpNI/edit#gid=938437500\"\n",
        "    sh = gc.open_by_url(SHEET_URL)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Google Sheets 認證或連線失敗: {e}\")\n",
        "    # 創建虛擬物件防止崩潰\n",
        "    class MockWorksheet:\n",
        "        def get_all_records(self): return []\n",
        "        def get_all_values(self): return []\n",
        "        def update(self, *args, **kwargs): pass\n",
        "        def clear(self): pass\n",
        "    ws_clips = MockWorksheet()\n",
        "    ws_analysis = MockWorksheet()\n",
        "\n",
        "\n",
        "# 確保工作表存在（\"爬蟲\"）\n",
        "def ensure_ws(name, headers):\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=name, rows=\"2000\", cols=str(len(headers)+10))\n",
        "        ws.update([headers])\n",
        "    # 如果沒有表頭就補上\n",
        "    vals = ws.get_all_values()\n",
        "    if not vals or (vals and vals[0] != headers):\n",
        "        ws.clear()\n",
        "        ws.update([headers])\n",
        "    return ws\n",
        "\n",
        "ws_clips = ensure_ws(\"爬蟲\", PTT_HEADER)            # 儲存抓到的文章（含 content）\n",
        "ws_analysis = ensure_ws(\"爬蟲_analysis\", [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"])\n",
        "\n",
        "# =========================\n",
        "# Gemini API 設定 (完全依照用戶指定的番茄鐘寫法)\n",
        "# =========================\n",
        "model = None # 將全域變數名稱設定為 model\n",
        "try:\n",
        "    # !!! 請替換為您真實的 API Key !!!\n",
        "    # 建議使用 Colab Secret 或環境變數儲存\n",
        "    GEMINI_API_KEY = \"AIzaSyAWyvSMGkAgiTMSdE8TEId8IFDw0OD46io\"\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    # 使用 Flash 模型，它速度快、成本低，適合規劃任務\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    print(\"✅ Gemini API 配置成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Gemini API 配置失敗: {e}\")\n",
        "\n",
        "# =========================\n",
        "# PTT 爬蟲函式\n",
        "# =========================\n",
        "PTT_INDEX = \"https://www.ptt.cc/bbs/China-Drama/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    for a in soup.select(\"div.btn-group-paging a.btn.wide\"):\n",
        "        if \"上頁\" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(span):\n",
        "    if not span:\n",
        "        return 0\n",
        "    txt = span.get_text(strip=True)\n",
        "    if txt == \"爆\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def extract_post_list_from_index(url):\n",
        "    soup = _get_soup(url)\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url_post = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\"title\": title, \"url\": url_post, \"author\": author, \"date\": date, \"nrec\": nrec})\n",
        "    prev = _get_prev_index_url(soup)\n",
        "    return posts, prev\n",
        "\n",
        "# 抓 index_pages 頁的文章列表（採去重）\n",
        "def crawl_index_pages(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    url = PTT_INDEX\n",
        "    all_rows = []\n",
        "    try:\n",
        "        seen = set([r['url'] for r in ws_clips.get_all_records()])\n",
        "    except Exception:\n",
        "        seen = set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        try:\n",
        "            posts, prev = extract_post_list_from_index(url)\n",
        "        except Exception as e:\n",
        "            return f\"⚠️ 取得 index 失敗：{e}\"\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and keyword not in p[\"title\"]:\n",
        "                continue\n",
        "            if p[\"url\"] in seen:\n",
        "                continue\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": p[\"title\"][:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": \"\"\n",
        "            })\n",
        "            seen.add(p[\"url\"])\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "    if all_rows:\n",
        "        df_new = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        # append to sheet\n",
        "        existing = get_as_dataframe(ws_clips, evaluate_formulas=False)\n",
        "        existing = existing.dropna(how=\"all\")\n",
        "        if existing is None or existing.empty:\n",
        "            combined = df_new\n",
        "        else:\n",
        "            combined = pd.concat([existing, df_new], ignore_index=True)\n",
        "        set_with_dataframe(ws_clips, combined)\n",
        "        return f\"✅ 取得 {len(all_rows)} 篇文章（寫入 '爬蟲'）\"\n",
        "    return \"ℹ️ 無新文章\"\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # 移除推文區與 meta\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\"\n",
        "    for m in main.select(\"div.article-metaline, div.article-metaline-right\"):\n",
        "        m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def fetch_and_write_contents(limit_per_run=50):\n",
        "    # 從 sheet 讀出尚未有 content 的 url（或 content 空白）\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df.empty:\n",
        "        return \"⚠️ '爬蟲' 工作表沒有資料\"\n",
        "    to_fetch = df[df[\"content\"].apply(lambda x: not bool(str(x).strip()))]\n",
        "    if to_fetch.empty:\n",
        "        return \"ℹ️ 沒有待抓取內文\"\n",
        "    updated = 0\n",
        "    for idx, row in to_fetch.head(limit_per_run).iterrows():\n",
        "        url = row[\"url\"]\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "            content = _clean_ptt_content(soup)\n",
        "        except Exception as e:\n",
        "            content = f\"FETCH_ERROR: {e}\" # 記錄錯誤\n",
        "        df.loc[idx, \"content\"] = content\n",
        "        df.loc[idx, \"fetched_at\"] = tznow().isoformat()\n",
        "        updated += 1\n",
        "        time.sleep(0.5)\n",
        "    set_with_dataframe(ws_clips, df)\n",
        "    return f\"✅ 已更新 {updated} 篇文章的內文到 '爬蟲' 工作表\"\n",
        "\n",
        "def load_sheet_preview(n=100):\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df is None:\n",
        "        return pd.DataFrame()\n",
        "    return df.head(n)\n",
        "\n",
        "# =========================\n",
        "# 文字分析 (分離出純分析和 AI 摘要邏輯)\n",
        "# =========================\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def simple_tokenize_cn(text):\n",
        "    # 只保留中文與英數，然後用 jieba 切詞\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", str(text))\n",
        "    toks = [w for w in jieba.lcut(text) if len(w.strip())>0]\n",
        "    return toks\n",
        "\n",
        "def get_analysis_data(df, topn, min_df, use_tfidf):\n",
        "    \"\"\"提取分析所需的數據，不執行寫入或 AI 呼叫\"\"\"\n",
        "    texts = df[\"content\"].astype(str).tolist()\n",
        "    docs = [t for t in texts if t.strip() and not t.startswith(\"FETCH_ERROR\")]\n",
        "\n",
        "    if not docs:\n",
        "        return None, None, None, \"⚠️ 沒有可分析的文字（'爬蟲' 工作表的 content 欄）\"\n",
        "\n",
        "    # 純文字分析部分 (詞頻/字數)\n",
        "    counts = [len(re.findall(r\"[\\u4e00-\\u9fffA-Za-z0-9]\", d)) for d in docs]\n",
        "    total_words = sum(counts)\n",
        "    avg_words = round(total_words / len(counts), 2)\n",
        "    token_docs = [simple_tokenize_cn(d) for d in docs]\n",
        "    freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        freq.update([t for t in toks if re.search(r'[\\u4e00-\\u9fff]', t)])\n",
        "\n",
        "    topn = int(topn)\n",
        "    common = freq.most_common(topn)\n",
        "    top_words_str = \"; \".join([f\"{w}:{c}\" for w,c in common])\n",
        "\n",
        "    # TF-IDF 可選\n",
        "    if use_tfidf:\n",
        "        try:\n",
        "            vec = TfidfVectorizer(tokenizer=simple_tokenize_cn, lowercase=False, min_df=min_df)\n",
        "            vec.fit_transform(docs)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    analysis_row = {\n",
        "        \"analysis_time\": tznow().isoformat(),\n",
        "        \"n_docs\": len(docs),\n",
        "        \"total_words\": total_words,\n",
        "        \"avg_words\": avg_words,\n",
        "        \"top_words\": top_words_str,\n",
        "        \"ai_summary\": \"\",\n",
        "        \"ai_conclusion\": \"\"\n",
        "    }\n",
        "\n",
        "    top_df = pd.DataFrame(common, columns=[\"word\",\"count\"])\n",
        "\n",
        "    return analysis_row, top_df, docs, f\"✅ 分析完成：{len(docs)} 篇；總字數 {total_words}；平均 {avg_words}\"\n",
        "\n",
        "def run_pure_analysis(topn, use_tfidf):\n",
        "    \"\"\"執行純文字分析並寫入 Google Sheet\"\"\"\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    analysis_row, top_df, docs, message = get_analysis_data(df, topn, 1, use_tfidf)\n",
        "\n",
        "    if analysis_row is None:\n",
        "        return message, pd.DataFrame(), \"(AI 摘要未執行)\"\n",
        "\n",
        "    # 寫入 ws_analysis (純文字分析結果)\n",
        "    analysis_header = [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"]\n",
        "    df_an = get_as_dataframe(ws_analysis, evaluate_formulas=False).fillna(\"\")\n",
        "\n",
        "    # 檢查並清理舊紀錄中的 AI 欄位，以確保不影響後續 AI 摘要的覆蓋\n",
        "    analysis_row[\"ai_summary\"] = \"（待 AI 摘要）\"\n",
        "    analysis_row[\"ai_conclusion\"] = \"\"\n",
        "\n",
        "    if df_an.empty or list(df_an.columns) != analysis_header:\n",
        "        ws_analysis.clear()\n",
        "        ws_analysis.update([analysis_header])\n",
        "        df_an_new = pd.DataFrame([analysis_row], columns=analysis_header)\n",
        "    else:\n",
        "        df_an_new = pd.concat([df_an, pd.DataFrame([analysis_row], columns=analysis_header)], ignore_index=True)\n",
        "    set_with_dataframe(ws_analysis, df_an_new)\n",
        "\n",
        "    # 寫入 top_words 到專門的 Sheet\n",
        "    top_ws = ensure_ws(\"爬蟲_top_words\", [\"word\",\"count\"])\n",
        "    set_with_dataframe(top_ws, top_df)\n",
        "\n",
        "    return message, top_df, \"(AI 摘要待執行)\"\n",
        "\n",
        "def run_ai_summary():\n",
        "    \"\"\"執行 AI 摘要，並更新 Google Sheet\"\"\"\n",
        "\n",
        "    global model\n",
        "    if model is None:\n",
        "        return \"❌ Gemini API 未配置，無法執行 AI 摘要。\", \"(AI 摘要未執行)\"\n",
        "\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "\n",
        "    # 使用預設參數取得內容和基本分析數據\n",
        "    analysis_row, top_df, docs, base_message = get_analysis_data(df, 20, 1, False)\n",
        "\n",
        "    if docs is None:\n",
        "         return base_message, \"(AI 摘要未執行)\"\n",
        "\n",
        "    ai_summary = \"⚠️ AI 摘要未執行\"\n",
        "    ai_conclusion = \"\"\n",
        "\n",
        "    try:\n",
        "        all_text_preview = \"\\n---\\n\".join(docs)[:10000]  # 擷取前 10000 字給模型\n",
        "\n",
        "        prompt = f\"\"\"請用中文，根據以下受訪者 / 文章內容，輸出：\n",
        "1) 五句簡短洞察（每句一行，最多 30字/句）\n",
        "2) 一段約 120 字的結論（總結情緒與主要趨勢）。\n",
        "內容：\n",
        "{all_text_preview}\n",
        "\"\"\"\n",
        "        # 呼叫 generate_content\n",
        "        resp = model.generate_content(prompt, request_options={\"timeout\": 60})\n",
        "        out = resp.text.strip()\n",
        "\n",
        "        parts = out.split('\\n\\n', 1)\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            ai_summary = parts[0].strip()\n",
        "            ai_conclusion = parts[1].strip()\n",
        "        else:\n",
        "            ai_summary = out.strip()\n",
        "            ai_conclusion = \"（無法從 AI 輸出中切分出明確的結論段落）\"\n",
        "\n",
        "    except Exception as e:\n",
        "        ai_summary = f\"⚠️ Gemini 呼叫失敗：{e}\"\n",
        "        ai_conclusion = \"\"\n",
        "\n",
        "    # 將 AI 結果寫入 ws_analysis (更新最近一筆紀錄)\n",
        "    df_an = get_as_dataframe(ws_analysis, evaluate_formulas=False).fillna(\"\")\n",
        "    analysis_header = [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"]\n",
        "\n",
        "    if not df_an.empty:\n",
        "        # 覆蓋最新一筆紀錄的 AI 欄位\n",
        "        df_an.loc[df_an.index[-1], \"ai_summary\"] = ai_summary\n",
        "        df_an.loc[df_an.index[-1], \"ai_conclusion\"] = ai_conclusion\n",
        "        set_with_dataframe(ws_analysis, df_an)\n",
        "\n",
        "        full_ai_output = f\"【洞察】\\n{ai_summary}\\n\\n【結論】\\n{ai_conclusion}\"\n",
        "        return f\"✅ AI 摘要已生成並更新 Google Sheet。\", full_ai_output\n",
        "    else:\n",
        "        # 如果沒有純分析紀錄，則創建新的一筆 (補上基礎數據)\n",
        "        if analysis_row is None:\n",
        "            analysis_row = {k: \"N/A\" for k in analysis_header}\n",
        "            analysis_row[\"analysis_time\"] = tznow().isoformat()\n",
        "\n",
        "        analysis_row[\"ai_summary\"] = ai_summary\n",
        "        analysis_row[\"ai_conclusion\"] = ai_conclusion\n",
        "\n",
        "        df_an_new = pd.DataFrame([analysis_row], columns=analysis_header)\n",
        "        set_with_dataframe(ws_analysis, df_an_new)\n",
        "\n",
        "        full_ai_output = f\"【洞察】\\n{ai_summary}\\n\\n【結論】\\n{ai_conclusion}\"\n",
        "        return f\"✅ AI 摘要已生成並寫入新紀錄。\", full_ai_output\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Gradio UI\n",
        "# =========================\n",
        "# Keep in-memory clip/task tables for Gradio display (your original)\n",
        "clips_df_local = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "tasks_df_local = pd.DataFrame(columns=TASKS_HEADER)\n",
        "\n",
        "with gr.Blocks(title=\"PTT Crawler & Task Integrator (extended)\") as demo:\n",
        "    gr.Markdown(\"## 🕸️ PTT China-Drama 爬蟲 + 內文抓取 + 文字分析（含 Gemini AI 摘要）\")\n",
        "\n",
        "    with gr.Tab(\"Crawler (index)\"):\n",
        "        url = gr.Textbox(label=\"起始 Index URL\", value=PTT_INDEX)\n",
        "        pages = gr.Number(label=\"向上抓幾頁 (index pages)\", value=3, precision=0)\n",
        "        min_push = gr.Number(label=\"最少推文數 (min_push)\", value=0, precision=0)\n",
        "        keyword = gr.Textbox(label=\"標題關鍵字過濾 (選填)\", value=\"\")\n",
        "        btn_index = gr.Button(\"📄 抓 Index（多頁）\")\n",
        "        out_index = gr.Markdown()\n",
        "        btn_index.click(fn=crawl_index_pages, inputs=[pages, min_push, keyword], outputs=[out_index])\n",
        "\n",
        "    with gr.Tab(\"抓內文（寫回 Sheet）\"):\n",
        "        btn_fetch = gr.Button(\"📥 抓取尚未抓內文的文章（寫回 '爬蟲'）\")\n",
        "        out_fetch = gr.Markdown()\n",
        "        btn_fetch.click(fn=fetch_and_write_contents, inputs=[gr.Number(value=50, visible=False)], outputs=[out_fetch])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        btn_show_sheet = gr.Button(\"📂 從 '爬蟲' 工作表讀出並顯示 (show)\")\n",
        "        sheet_table = gr.Dataframe(value=pd.DataFrame(), interactive=True)\n",
        "\n",
        "        btn_show_sheet.click(fn=load_sheet_preview, outputs=[sheet_table])\n",
        "\n",
        "    with gr.Tab(\"Add to Tasks\"):\n",
        "        clip_ids = gr.Textbox(label=\"要加入任務的 post_id（多個以逗號分隔）\")\n",
        "        default_priority = gr.Radio([\"H\", \"M\", \"L\"], value=\"M\", label=\"預設優先度\")\n",
        "        est_min = gr.Number(value=25, precision=0, label=\"預估時間（分鐘）\")\n",
        "        btn_add = gr.Button(\"➕ 加入任務\")\n",
        "        msg_add = gr.Markdown()\n",
        "        grid_tasks = gr.Dataframe(value=tasks_df_local, label=\"任務清單\", interactive=True)\n",
        "\n",
        "        # local add function\n",
        "        def add_clips_local(clip_ids, default_priority, est_min):\n",
        "            global tasks_df_local\n",
        "            if not clip_ids:\n",
        "                return \"⚠️ 請輸入 post_id\", load_sheet_preview(), tasks_df_local\n",
        "            ids = [c.strip() for c in clip_ids.split(\",\") if c.strip()]\n",
        "\n",
        "            df_sheet = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "            sel = df_sheet[df_sheet[\"post_id\"].isin(ids)] if not df_sheet.empty else pd.DataFrame()\n",
        "\n",
        "            if sel.empty:\n",
        "                return \"⚠️ 沒有匹配到任何 post_id\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "            _now = tznow().isoformat()\n",
        "            new_tasks = []\n",
        "            for _, r in sel.iterrows():\n",
        "                title = r.get(\"title\", \"\") or \"（未命名）\"\n",
        "                note = f\"PTT 連結：{r.get('url','')}\\n作者：{r.get('author','')}\\n原始 ID：{r.get('post_id','')}\"\n",
        "                new_tasks.append({\n",
        "                    \"id\": str(uuid.uuid4())[:8],\n",
        "                    \"task\": title[:120],\n",
        "                    \"status\": \"todo\",\n",
        "                    \"priority\": default_priority or \"M\",\n",
        "                    \"est_min\": int(est_min) if est_min else 25,\n",
        "                    \"start_time\": \"\",\n",
        "                    \"end_time\": \"\",\n",
        "                    \"actual_min\": 0,\n",
        "                    \"pomodoros\": 0,\n",
        "                    \"due_date\": \"\",\n",
        "                    \"labels\": \"from:ptt\",\n",
        "                    \"notes\": note,\n",
        "                    \"created_at\": _now,\n",
        "                    \"updated_at\": _now,\n",
        "                    \"completed_at\": \"\",\n",
        "                    \"planned_for\": \"\"\n",
        "                })\n",
        "            if new_tasks:\n",
        "                tasks_df_local = pd.concat([tasks_df_local, pd.DataFrame(new_tasks, columns=TASKS_HEADER)], ignore_index=True)\n",
        "                return f\"✅ 已加入 {len(new_tasks)} 筆任務 (local)\", load_sheet_preview(), tasks_df_local\n",
        "            return \"⚠️ 沒有可加入的項目\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "\n",
        "        btn_add.click(fn=add_clips_local, inputs=[clip_ids, default_priority, est_min], outputs=[msg_add, sheet_table, grid_tasks])\n",
        "\n",
        "    with gr.Tab(\"🧠 Text Analysis (Gemini)\"):\n",
        "        with gr.Row():\n",
        "            topn = gr.Number(label=\"Top N 熱詞\", value=20, precision=0, scale=0)\n",
        "            use_tfidf = gr.Checkbox(label=\"使用 TF-IDF (補充排序)\", value=False, scale=1)\n",
        "\n",
        "        # 分離按鈕 1: 純文字分析\n",
        "        btn_analyze = gr.Button(\"🔍 1. 執行純文字分析 (詞頻/字數)\")\n",
        "        result_msg = gr.Markdown(value=\"---\")\n",
        "\n",
        "        # 移除 height 參數\n",
        "        top_table = gr.Dataframe(label=\"熱門詞彙 Top N\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # 分離按鈕 2: AI 摘要\n",
        "        btn_ai_summary = gr.Button(\"🤖 2. 執行 AI 摘要 (呼叫 Gemini)\")\n",
        "        ai_msg = gr.Markdown(value=\"---\")\n",
        "\n",
        "        # AI 輸出欄位\n",
        "        ai_out = gr.Textbox(label=\"Gemini AI 摘要 (洞察 + 結論)\", lines=10, interactive=False)\n",
        "\n",
        "        # 事件綁定 1: 純文字分析\n",
        "        # 輸出：純分析結果訊息, 熱詞表格, AI 摘要佔位符\n",
        "        btn_analyze.click(fn=run_pure_analysis,\n",
        "                          inputs=[topn, use_tfidf],\n",
        "                          outputs=[result_msg, top_table, ai_out])\n",
        "\n",
        "        # 事件綁定 2: AI 摘要\n",
        "        # 輸出：AI 摘要訊息, AI 輸出文字\n",
        "        btn_ai_summary.click(fn=run_ai_summary,\n",
        "                             inputs=None,\n",
        "                             outputs=[ai_msg, ai_out])\n",
        "\n",
        "# ====== 啟動 App ======\n",
        "if __name__ == \"__main__\":\n",
        "    # 完全使用您指定的參數\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "45orI7hgD2jn",
        "outputId": "434013cb-b7d4-4c28-fe6d-5cc595f159c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini API 配置成功。\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4031eca500f5c719f3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4031eca500f5c719f3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.813 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.813 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4031eca500f5c719f3.gradio.live\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXp9a4/G4xiehCeGMdeHcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}