{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPshCS5jnhG3yncaeJj3b+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371116h/PL-Repo./blob/main/HW4%E5%90%ABAI%E6%91%98%E8%A6%81_(PTT_China_Drama).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 🕸️ PTT China-Drama 爬蟲 + 任務整合 + 內文抓取 + 文字分析（含寫回 Google Sheet & AI 摘要）\n",
        "# 🚨 Gemini 配置區塊已完全替換為用戶指定的寫法 🚨\n",
        "# ============================================\n",
        "# 安裝（Colab 執行一次）\n",
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 python-dateutil jieba google-generativeai\n",
        "\n",
        "# -------------------------\n",
        "import os, time, uuid, re, datetime\n",
        "import requests, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from collections import Counter, defaultdict\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "import gspread\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "from io import StringIO\n",
        "\n",
        "# =========================\n",
        "# 時間工具\n",
        "# =========================\n",
        "def tznow():\n",
        "    return datetime.datetime.now().astimezone()\n",
        "\n",
        "# =========================\n",
        "# 常數 / 表頭\n",
        "# =========================\n",
        "CLIPS_HEADER = [\"clip_id\",\"url\",\"selector\",\"text\",\"href\",\"created_at\",\"added_to_task\"]\n",
        "TASKS_HEADER = [\"id\",\"task\",\"status\",\"priority\",\"est_min\",\"start_time\",\"end_time\",\n",
        "                \"actual_min\",\"pomodoros\",\"due_date\",\"labels\",\"notes\",\n",
        "                \"created_at\",\"updated_at\",\"completed_at\",\"planned_for\"]\n",
        "PTT_HEADER = [\"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\"fetched_at\",\"content\"]\n",
        "\n",
        "# =========================\n",
        "# Google Sheets 初始化（Colab 認證）\n",
        "# =========================\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    # 把你的試算表 URL 放這裡\n",
        "    SHEET_URL = \"https://docs.google.com/spreadsheets/d/1GScHTHISiioV89XO5twgGl-IpaYG-0nMwhLRTizSpNI/edit#gid=938437500\"\n",
        "    sh = gc.open_by_url(SHEET_URL)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Google Sheets 認證或連線失敗: {e}\")\n",
        "    # 創建虛擬物件防止崩潰\n",
        "    class MockWorksheet:\n",
        "        def get_all_records(self): return []\n",
        "        def get_all_values(self): return []\n",
        "        def update(self, *args, **kwargs): pass\n",
        "        def clear(self): pass\n",
        "    ws_clips = MockWorksheet()\n",
        "    ws_analysis = MockWorksheet()\n",
        "\n",
        "\n",
        "# 確保工作表存在（\"爬蟲\"）\n",
        "def ensure_ws(name, headers):\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=name, rows=\"2000\", cols=str(len(headers)+10))\n",
        "        ws.update([headers])\n",
        "    # 如果沒有表頭就補上\n",
        "    vals = ws.get_all_values()\n",
        "    if not vals or (vals and vals[0] != headers):\n",
        "        ws.clear()\n",
        "        ws.update([headers])\n",
        "    return ws\n",
        "\n",
        "ws_clips = ensure_ws(\"爬蟲\", PTT_HEADER)            # 儲存抓到的文章（含 content）\n",
        "ws_analysis = ensure_ws(\"爬蟲_analysis\", [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"])\n",
        "\n",
        "# =========================\n",
        "# Gemini API 設定 (完全依照用戶指定的番茄鐘寫法)\n",
        "# =========================\n",
        "model = None # 將全域變數名稱設定為 model\n",
        "try:\n",
        "    # !!! 請替換為您真實的 API Key !!!\n",
        "    # 建議使用 Colab Secret 或環境變數儲存\n",
        "    GEMINI_API_KEY = \"AIzaSyAWyvSMGkAgiTMSdE8TEId8IFDw0OD46io\"\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    # 使用 Flash 模型，它速度快、成本低，適合規劃任務\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    print(\"✅ Gemini API 配置成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Gemini API 配置失敗: {e}\")\n",
        "\n",
        "# =========================\n",
        "# PTT 爬蟲函式\n",
        "# =========================\n",
        "PTT_INDEX = \"https://www.ptt.cc/bbs/China-Drama/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    for a in soup.select(\"div.btn-group-paging a.btn.wide\"):\n",
        "        if \"上頁\" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(span):\n",
        "    if not span:\n",
        "        return 0\n",
        "    txt = span.get_text(strip=True)\n",
        "    if txt == \"爆\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def extract_post_list_from_index(url):\n",
        "    soup = _get_soup(url)\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url_post = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\"title\": title, \"url\": url_post, \"author\": author, \"date\": date, \"nrec\": nrec})\n",
        "    prev = _get_prev_index_url(soup)\n",
        "    return posts, prev\n",
        "\n",
        "# 抓 index_pages 頁的文章列表（採去重）\n",
        "def crawl_index_pages(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    url = PTT_INDEX\n",
        "    all_rows = []\n",
        "    try:\n",
        "        seen = set([r['url'] for r in ws_clips.get_all_records()])\n",
        "    except Exception:\n",
        "        seen = set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        try:\n",
        "            posts, prev = extract_post_list_from_index(url)\n",
        "        except Exception as e:\n",
        "            return f\"⚠️ 取得 index 失敗：{e}\"\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and keyword not in p[\"title\"]:\n",
        "                continue\n",
        "            if p[\"url\"] in seen:\n",
        "                continue\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": p[\"title\"][:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": \"\"\n",
        "            })\n",
        "            seen.add(p[\"url\"])\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "    if all_rows:\n",
        "        df_new = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        # append to sheet\n",
        "        existing = get_as_dataframe(ws_clips, evaluate_formulas=False)\n",
        "        existing = existing.dropna(how=\"all\")\n",
        "        if existing is None or existing.empty:\n",
        "            combined = df_new\n",
        "        else:\n",
        "            combined = pd.concat([existing, df_new], ignore_index=True)\n",
        "        set_with_dataframe(ws_clips, combined)\n",
        "        return f\"✅ 取得 {len(all_rows)} 篇文章（寫入 '爬蟲'）\"\n",
        "    return \"ℹ️ 無新文章\"\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # 移除推文區與 meta\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\"\n",
        "    for m in main.select(\"div.article-metaline, div.article-metaline-right\"):\n",
        "        m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    return text\n",
        "\n",
        "def fetch_and_write_contents(limit_per_run=50):\n",
        "    # 從 sheet 讀出尚未有 content 的 url（或 content 空白）\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df.empty:\n",
        "        return \"⚠️ '爬蟲' 工作表沒有資料\"\n",
        "    to_fetch = df[df[\"content\"].apply(lambda x: not bool(str(x).strip()))]\n",
        "    if to_fetch.empty:\n",
        "        return \"ℹ️ 沒有待抓取內文\"\n",
        "    updated = 0\n",
        "    for idx, row in to_fetch.head(limit_per_run).iterrows():\n",
        "        url = row[\"url\"]\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "            content = _clean_ptt_content(soup)\n",
        "        except Exception as e:\n",
        "            content = f\"FETCH_ERROR: {e}\" # 記錄錯誤\n",
        "        df.loc[idx, \"content\"] = content\n",
        "        df.loc[idx, \"fetched_at\"] = tznow().isoformat()\n",
        "        updated += 1\n",
        "        time.sleep(0.5)\n",
        "    set_with_dataframe(ws_clips, df)\n",
        "    return f\"✅ 已更新 {updated} 篇文章的內文到 '爬蟲' 工作表\"\n",
        "\n",
        "def load_sheet_preview(n=100):\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    if df is None:\n",
        "        return pd.DataFrame()\n",
        "    return df.head(n)\n",
        "\n",
        "# =========================\n",
        "# 文字分析（使用 Gemini API 進行摘要）\n",
        "# =========================\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def simple_tokenize_cn(text):\n",
        "    # 只保留中文與英數，然後用 jieba 切詞\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", str(text))\n",
        "    toks = [w for w in jieba.lcut(text) if len(w.strip())>0]\n",
        "    return toks\n",
        "\n",
        "def analyze_and_write(topn=20, min_df=1, use_tfidf=False):\n",
        "    # 讀取 sheet '爬蟲' 的 content 欄\n",
        "    df = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "    texts = df[\"content\"].astype(str).tolist()\n",
        "    # 排除空內容或錯誤訊息 (FETCH_ERROR)\n",
        "    docs = [t for t in texts if t.strip() and not t.startswith(\"FETCH_ERROR\")]\n",
        "    if not docs:\n",
        "        return \"⚠️ 沒有可分析的文字（'爬蟲' 工作表的 content 欄）\", pd.DataFrame(), \"\"\n",
        "\n",
        "    # 純文字分析部分 (詞頻/字數)\n",
        "    counts = [len(re.findall(r\"[\\u4e00-\\u9fffA-Za-z0-9]\", d)) for d in docs]\n",
        "    total_words = sum(counts)\n",
        "    avg_words = round(total_words / len(counts), 2)\n",
        "    token_docs = [simple_tokenize_cn(d) for d in docs]\n",
        "    freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        freq.update([t for t in toks if re.search(r'[\\u4e00-\\u9fff]', t)])  # 優先中文詞\n",
        "    topn = int(topn)\n",
        "    common = freq.most_common(topn)\n",
        "    top_words_str = \"; \".join([f\"{w}:{c}\" for w,c in common])\n",
        "\n",
        "    # TF-IDF 可選（計算但不影響 top_words_str）\n",
        "    if use_tfidf:\n",
        "        try:\n",
        "            vec = TfidfVectorizer(tokenizer=simple_tokenize_cn, lowercase=False, min_df=min_df)\n",
        "            vec.fit_transform(docs)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ==========================================================\n",
        "    # AI 生成摘要 (使用全域變數 model)\n",
        "    # ==========================================================\n",
        "    ai_summary = \"⚠️ AI 摘要未執行\"\n",
        "    ai_conclusion = \"\"\n",
        "\n",
        "    global model # 使用番茄鐘專案設定的全域變數 model\n",
        "    if model is not None:\n",
        "        try:\n",
        "            all_text_preview = \"\\n---\\n\".join(docs)[:10000]  # 擷取前 10000 字給模型\n",
        "\n",
        "            # 使用您指定的 Prompt 格式\n",
        "            prompt = f\"\"\"請用中文，根據以下受訪者 / 文章內容，輸出：\n",
        "1) 五句簡短洞察（每句一行，最多 30字/句）\n",
        "2) 一段約 120 字的結論（總結情緒與主要趨勢）。\n",
        "內容：\n",
        "{all_text_preview}\n",
        "\"\"\"\n",
        "            # 呼叫 generate_content\n",
        "            resp = model.generate_content(prompt, request_options={\"timeout\": 60})\n",
        "            out = resp.text.strip()\n",
        "\n",
        "            # 嘗試將輸出切分成 summary 和 conclusion (簡單切分)\n",
        "            parts = out.split('\\n\\n', 1) # 只切分一次\n",
        "\n",
        "            if len(parts) == 2:\n",
        "                ai_summary = parts[0].strip()\n",
        "                ai_conclusion = parts[1].strip()\n",
        "            else:\n",
        "                ai_summary = out.strip()\n",
        "                ai_conclusion = \"（無法從 AI 輸出中切分出明確的結論段落）\"\n",
        "\n",
        "        except Exception as e:\n",
        "            ai_summary = f\"⚠️ Gemini 呼叫失敗：{e}\"\n",
        "            ai_conclusion = \"\"\n",
        "\n",
        "    # 回寫 ws_analysis（新一列）\n",
        "    analysis_row = {\n",
        "        \"analysis_time\": tznow().isoformat(),\n",
        "        \"n_docs\": len(docs),\n",
        "        \"total_words\": total_words,\n",
        "        \"avg_words\": avg_words,\n",
        "        \"top_words\": top_words_str,\n",
        "        \"ai_summary\": ai_summary,\n",
        "        \"ai_conclusion\": ai_conclusion\n",
        "    }\n",
        "\n",
        "    analysis_header = [\"analysis_time\",\"n_docs\",\"total_words\",\"avg_words\",\"top_words\",\"ai_summary\",\"ai_conclusion\"]\n",
        "    df_an = get_as_dataframe(ws_analysis, evaluate_formulas=False).fillna(\"\")\n",
        "\n",
        "    if df_an.empty or list(df_an.columns) != analysis_header:\n",
        "        ws_analysis.clear()\n",
        "        ws_analysis.update([analysis_header])\n",
        "        df_an_new = pd.DataFrame([analysis_row], columns=analysis_header)\n",
        "    else:\n",
        "        df_an_new = pd.concat([df_an, pd.DataFrame([analysis_row], columns=analysis_header)], ignore_index=True)\n",
        "    set_with_dataframe(ws_analysis, df_an_new)\n",
        "\n",
        "    # 也把 top words 寫到一個 sheet \"爬蟲_top_words\"（覆蓋）\n",
        "    top_ws = ensure_ws(\"爬蟲_top_words\", [\"word\",\"count\"])\n",
        "    top_df = pd.DataFrame(common, columns=[\"word\",\"count\"])\n",
        "    set_with_dataframe(top_ws, top_df)\n",
        "\n",
        "    # 將 summary 和 conclusion 合併顯示在 Gradio 的 ai_out\n",
        "    full_ai_output = f\"【洞察】\\n{ai_summary}\\n\\n【結論】\\n{ai_conclusion}\"\n",
        "\n",
        "    return {\n",
        "        \"message\": f\"✅ 分析完成：{len(docs)} 篇；總字數 {total_words}；平均 {avg_words}\",\n",
        "        \"top_df\": top_df,\n",
        "        \"ai_summary\": full_ai_output\n",
        "    }\n",
        "\n",
        "# Gradio Wrapper for analysis (調整輸出)\n",
        "def run_analysis(topn, use_tfidf):\n",
        "    res = analyze_and_write(topn, min_df=1, use_tfidf=use_tfidf)\n",
        "    if isinstance(res, dict):\n",
        "        return res[\"message\"], res[\"top_df\"], res[\"ai_summary\"]\n",
        "    return res, pd.DataFrame(), f\"分析失敗: {res}\"\n",
        "\n",
        "# =========================\n",
        "# Gradio UI\n",
        "# =========================\n",
        "# Keep in-memory clip/task tables for Gradio display (your original)\n",
        "clips_df_local = pd.DataFrame(columns=CLIPS_HEADER)\n",
        "tasks_df_local = pd.DataFrame(columns=TASKS_HEADER)\n",
        "\n",
        "with gr.Blocks(title=\"PTT Crawler & Task Integrator (extended)\") as demo:\n",
        "    gr.Markdown(\"## 🕸️ PTT China-Drama 爬蟲 + 內文抓取 + 文字分析（含 Gemini AI 摘要）\")\n",
        "\n",
        "    with gr.Tab(\"Crawler (index)\"):\n",
        "        url = gr.Textbox(label=\"起始 Index URL\", value=PTT_INDEX)\n",
        "        pages = gr.Number(label=\"向上抓幾頁 (index pages)\", value=3, precision=0)\n",
        "        min_push = gr.Number(label=\"最少推文數 (min_push)\", value=0, precision=0)\n",
        "        keyword = gr.Textbox(label=\"標題關鍵字過濾 (選填)\", value=\"\")\n",
        "        btn_index = gr.Button(\"📄 抓 Index（多頁）\")\n",
        "        out_index = gr.Markdown()\n",
        "        btn_index.click(fn=crawl_index_pages, inputs=[pages, min_push, keyword], outputs=[out_index])\n",
        "\n",
        "    with gr.Tab(\"抓內文（寫回 Sheet）\"):\n",
        "        btn_fetch = gr.Button(\"📥 抓取尚未抓內文的文章（寫回 '爬蟲'）\")\n",
        "        out_fetch = gr.Markdown()\n",
        "        btn_fetch.click(fn=fetch_and_write_contents, inputs=[gr.Number(value=50, visible=False)], outputs=[out_fetch])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        btn_show_sheet = gr.Button(\"📂 從 '爬蟲' 工作表讀出並顯示 (show)\")\n",
        "        sheet_table = gr.Dataframe(value=pd.DataFrame(), interactive=True)\n",
        "\n",
        "        btn_show_sheet.click(fn=load_sheet_preview, outputs=[sheet_table])\n",
        "\n",
        "    with gr.Tab(\"Add to Tasks\"):\n",
        "        clip_ids = gr.Textbox(label=\"要加入任務的 post_id（多個以逗號分隔）\")\n",
        "        default_priority = gr.Radio([\"H\", \"M\", \"L\"], value=\"M\", label=\"預設優先度\")\n",
        "        est_min = gr.Number(value=25, precision=0, label=\"預估時間（分鐘）\")\n",
        "        btn_add = gr.Button(\"➕ 加入任務\")\n",
        "        msg_add = gr.Markdown()\n",
        "        grid_tasks = gr.Dataframe(value=tasks_df_local, label=\"任務清單\", interactive=True)\n",
        "\n",
        "        # local add function\n",
        "        def add_clips_local(clip_ids, default_priority, est_min):\n",
        "            global tasks_df_local\n",
        "            if not clip_ids:\n",
        "                return \"⚠️ 請輸入 post_id\", load_sheet_preview(), tasks_df_local\n",
        "            ids = [c.strip() for c in clip_ids.split(\",\") if c.strip()]\n",
        "\n",
        "            df_sheet = get_as_dataframe(ws_clips, evaluate_formulas=False).fillna(\"\")\n",
        "            sel = df_sheet[df_sheet[\"post_id\"].isin(ids)] if not df_sheet.empty else pd.DataFrame()\n",
        "\n",
        "            if sel.empty:\n",
        "                return \"⚠️ 沒有匹配到任何 post_id\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "            _now = tznow().isoformat()\n",
        "            new_tasks = []\n",
        "            for _, r in sel.iterrows():\n",
        "                title = r.get(\"title\", \"\") or \"（未命名）\"\n",
        "                note = f\"PTT 連結：{r.get('url','')}\\n作者：{r.get('author','')}\\n原始 ID：{r.get('post_id','')}\"\n",
        "                new_tasks.append({\n",
        "                    \"id\": str(uuid.uuid4())[:8],\n",
        "                    \"task\": title[:120],\n",
        "                    \"status\": \"todo\",\n",
        "                    \"priority\": default_priority or \"M\",\n",
        "                    \"est_min\": int(est_min) if est_min else 25,\n",
        "                    \"start_time\": \"\",\n",
        "                    \"end_time\": \"\",\n",
        "                    \"actual_min\": 0,\n",
        "                    \"pomodoros\": 0,\n",
        "                    \"due_date\": \"\",\n",
        "                    \"labels\": \"from:ptt\",\n",
        "                    \"notes\": note,\n",
        "                    \"created_at\": _now,\n",
        "                    \"updated_at\": _now,\n",
        "                    \"completed_at\": \"\",\n",
        "                    \"planned_for\": \"\"\n",
        "                })\n",
        "            if new_tasks:\n",
        "                tasks_df_local = pd.concat([tasks_df_local, pd.DataFrame(new_tasks, columns=TASKS_HEADER)], ignore_index=True)\n",
        "                return f\"✅ 已加入 {len(new_tasks)} 筆任務 (local)\", load_sheet_preview(), tasks_df_local\n",
        "            return \"⚠️ 沒有可加入的項目\", load_sheet_preview(), tasks_df_local\n",
        "\n",
        "\n",
        "        btn_add.click(fn=add_clips_local, inputs=[clip_ids, default_priority, est_min], outputs=[msg_add, sheet_table, grid_tasks])\n",
        "\n",
        "    with gr.Tab(\"🧠 Text Analysis (Gemini)\"):\n",
        "        topn = gr.Number(label=\"Top N 熱詞\", value=20, precision=0)\n",
        "        use_tfidf = gr.Checkbox(label=\"使用 TF-IDF (補充排序)\", value=False)\n",
        "        btn_analyze = gr.Button(\"🔍 分析（從 '爬蟲' 讀取 content，並寫回 analysis）\")\n",
        "        result_msg = gr.Markdown()\n",
        "        top_table = gr.Dataframe()\n",
        "        # AI 輸出欄位調整為 Lines=10\n",
        "        ai_out = gr.Textbox(label=\"Gemini AI 摘要 (洞察 + 結論)\", lines=10, interactive=False)\n",
        "\n",
        "        btn_analyze.click(fn=run_analysis, inputs=[topn, use_tfidf], outputs=[result_msg, top_table, ai_out])\n",
        "\n",
        "# ====== 啟動 App ======\n",
        "if __name__ == \"__main__\":\n",
        "    # 完全使用您指定的參數\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "2HfAA2yuS473",
        "outputId": "f156eb35-b245-4395-a4af-a6ef7e97eee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini API 配置成功。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f248fe8b3c58851991.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f248fe8b3c58851991.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.542 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.542 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    }
  ]
}